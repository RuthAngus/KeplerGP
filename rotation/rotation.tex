\documentclass[12pt,preprint]{aastex}
\usepackage{amsmath}
\usepackage{breqn}
\usepackage{cite,natbib}
\usepackage{epsfig}
\usepackage{cases}
\usepackage[section]{placeins}
\usepackage{graphicx, subfigure}

\begin{document}

\title{Measuring Stellar rotation periods with Gaussian Processes}

\begin{abstract}

None of the commonly used methods for measuring periodic signals are ideally suited to photometric stellar rotation periods.
% Of the most commonly used methods for measuring photometric stellar rotation periods, none is ideally suited to the typically quasi-periodic, non-sinusoidal rotation signals.
% The most commonly used methods for measuring photometric stellar rotation periods are not ideally suited to the problem.
Periodogram and wavelet based methods are not suitable for quasi-periodic, non-sinusoidal signals.
They often cannot distinguish between 'true' periods and harmonics and tend to fail in cases where systematics dominate or substantially contribute to the signal.
The autocorrelation function (ACF) method, (McQuillan et al, 2012) \emph{is} able to distinguish a true period from its harmonics in most cases and is well adapted to quasi-periodic, non-sinusiodal signals.
However, in general it requires heuristics --- hand-tuned, non-motivated-by-science parameters, to determine the `reliability' of a rotation period measurement.
General practise within the field is to use the ACF method, or some combination of periodogram and wavelets. % is this true?
The major drawback to all of the methods listed above, is that uncertainties on rotation periods are difficult to quantify.
The ideal method for measuring stellar rotation periods would be to use a physically-motivated model and fit for its parameters.
Unfortunately, the physics driving the production of variability in stellar light curves is poorly understood and parameters in existing spots models are highly degenerate.
In particular, spot lifetime and differential rotation (shear) are highly degenerate.
In the absence of a well-motivated physical model, we opt to use a highly flexible, semi-parametric model: a Gaussian process (GP).
It is semi-parametric because its parameters do not apply to the light curve itself, but to the properties of the covariance structure of the light curve.
A GP model can cope with quasi-periodic, non-sinusoidal signals, it can model systematics and physical signals simultaneously and provides rotation period measurements with well estimated uncertainties.

\end{abstract}

\section{Introduction}

Thanks to the high-precision photometry provided by Kepler, stellar rotation periods can now be measured directly from light curves.
% Spotted, rotating stars produce variable light curves due to the repeated
The nature of this variability tends to be non-sinusoidal --- star spot patterns are irregular.
Stars do not rotate as solid bodys --- they have different rotation periods at different latitudes.
The rotation period of whatever annulus upon which the most dominant spot region lies will show up in the light curve.
Sun spots are born at mid-latitudes and migrate towards the equator.
If this is also assumed to happen in solar-type stars, this effect will make the stellar rotation period seem to vary.
In general it is also quasi-periodic: stars

A Gaussian process is any stochastic process in which the joint probability distribution over N samples drawn from the process is Gaussian in N dimensions.
GPs are commonly used in the fields of machine learning, information engineering, biology and cosmology.
They are now beginning to be used in the field of time domain astronomy (Gibson et al, 2011, Aigrain et al, 2012, Foreman-Mackey et al, 2014).
The popularity of GPs is growing within time domain astronomy now that fast GP methods are being developed (cite Dan's paper), as previously, especially for Kepler data, they have been prohibitively computationally expensive.

\section{Method}

We use a GP to model stellar variability in Kepler light curves.

The kernel function parameterises the covariance matrix.
The only requirement for a kernel function to be valid is that it produces a kernel function that is symmetric and positive semi-definite.
Some examples of commonly used kernel functions include the squared exponential:

\begin{equation}
	k_{ij} = A\exp\left[{\frac{-(x_i-x_j)}{2l^2}}\right]
\end{equation}

which is the simplest kernel you can write down --- it has only two parameters, an amplitude, A and a length scale, l.
The length scale parameter controls the rate at which the covariance between data points falls away.
Large $l$ will produce a slowly varying mean function where points are correlated with many surrounding data points.
Small $l$ will produce a rapidly varying mean function, where data points are only correlated with their immediate neighbours.

Kernel functions can be periodic,  --- a simple periodic function might look like the following:

\begin{equation}
	k_{ij} = A\exp\left[{\frac{-\sin^2\left({\frac{\pi(x_i-x_j)}{P})}\right)}{L^2}}\right],
\end{equation}

where P is the period and L is something like a characteristic length scale in phase-space.
This periodic kernel function parameterises the covariance between points of the same phase; only points with similar phases will be highly correlated.

Kernel functions have the fantastic property that they can be added or multiplied together to produce other valid kernel functions.
To construct our quasi-periodic kernel function we multiply a simple periodic kernel with a squared exponential,

\begin{equation}
	k_{ij} = A\exp\left[{\frac{-(x_i-x_j)}{2l^2}}\right]\exp\left[\frac{-\sin^2\left(\frac{\pi(x_i-x_j)}{P}\right)}{L^2}\right],
\end{equation}

Given a data set, $\mathbf{x_n} = {x_1, x_2, ..., x_n}$, $\mathbf{y_n} = {y_1, y_2, ..., y_n}$, where in our specific case $x$ is time and $y$ is flux, the log-likelihood function will be as follows:

\begin{equation}
	\ln{\mathcal{L}} = -\frac{1}{2}~\left(\mathbf{y_n}^T~K^{-1}~\mathbf{y_n}~ + ~\log{|K| ~+~ n\log(2\pi)} \right),
\end{equation}

where K is the covariance matrix, constructed from $\mathbf{x_n}$ and the selected covariance kernel.

\subsection{Computational tractability}

Computing a GP likelihood can be expensive as the determinant and inverse of the covariance matrix must be calculated (although, in reality the inverse is never actually computed).
Naively, these operations scale like $N^3$.
In practise this covariance matrix can be very large --- for example, one quarter of long cadence Kepler data has around 4000 data points and will produce a 4000x4000 covariance matrix.
For the specific problem of rotation period measurement, the likelihood is likely to be very multi-modal and we therefore opt to sample from the posterior using MCMC.
This means that the determininant and inverse of a very large covariance matrix must be calculated once per likelihood call.
It is easy to see that this problem could very quickly become intractable.

Luckily there are some tricks we can employ to speed the process up. %thank god!
Decomposing the matrix into smaller matrices, e.g. using a Cholesky decomposition can speed up the process a little.
Talk more about the methods used in George.
Scales as $N\log^2{N}$.
Circulant method scales as $N\log{N}$.

\section{Future work}

Figure out how to join quarters together with a GP.

\end{document}
